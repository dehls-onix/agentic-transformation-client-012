{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Customer LTV Analysis with In-Database Feature Engineering\n",
    "\n",
    "**Objective:** Predict Customer Lifetime Value (LTV) by creating advanced features directly in Snowflake. This notebook uses a complex SQL query with CTEs and window functions to aggregate customer behavior before training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Libraries loaded for advanced analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse='ANALYST_WH',\n",
    "    database='RETAIL_PROD',\n",
    "    schema='ANALYTICS'\n",
    ")\n",
    "print(\"Snowflake connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This single, complex query performs all feature engineering in Snowflake.\n",
    "feature_engineering_sql = \"\"\"\n",
    "WITH customer_transactions AS (\n",
    "    -- First, get transaction details and calculate days since last purchase\n",
    "    SELECT\n",
    "        CUSTOMER_ID,\n",
    "        TRANSACTION_ID,\n",
    "        TRANSACTION_DATE,\n",
    "        AMOUNT,\n",
    "        DATEDIFF('day', TRANSACTION_DATE, CURRENT_DATE) as DAYS_SINCE_TRANSACTION\n",
    "    FROM TRANSACTIONS\n",
    "),\n",
    "\n",
    "customer_aggregates AS (\n",
    "    -- Next, aggregate key metrics for each customer\n",
    "    SELECT\n",
    "        CUSTOMER_ID,\n",
    "        COUNT(TRANSACTION_ID) as TOTAL_TRANSACTIONS,\n",
    "        SUM(AMOUNT) as TOTAL_SPEND,\n",
    "        AVG(AMOUNT) as AVG_TRANSACTION_VALUE,\n",
    "        MIN(DAYS_SINCE_TRANSACTION) as RECENCY -- Days since their most recent purchase\n",
    "    FROM customer_transactions\n",
    "    GROUP BY CUSTOMER_ID\n",
    "),\n",
    "\n",
    "customer_cohorts AS (\n",
    "    -- Finally, join with customer info and add window function features\n",
    "    SELECT\n",
    "        c.CUSTOMER_ID,\n",
    "        c.TENURE_MONTHS,\n",
    "        c.PREFERRED_CHANNEL,\n",
    "        agg.TOTAL_TRANSACTIONS,\n",
    "        agg.TOTAL_SPEND as LTV_TARGET, -- This is our prediction target\n",
    "        agg.AVG_TRANSACTION_VALUE,\n",
    "        agg.RECENCY,\n",
    "        -- Calculate percentile rank for spend within their channel cohort\n",
    "        PERCENT_RANK() OVER (PARTITION BY c.PREFERRED_CHANNEL ORDER BY agg.TOTAL_SPEND) as SPEND_PERCENTILE_IN_CHANNEL\n",
    "    FROM CUSTOMERS c\n",
    "    JOIN customer_aggregates agg ON c.CUSTOMER_ID = agg.CUSTOMER_ID\n",
    ")\n",
    "\n",
    "SELECT * FROM customer_cohorts;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing advanced feature engineering query in Snowflake...\")\n",
    "ltv_features_df = pd.read_sql(feature_engineering_sql, conn)\n",
    "conn.close()\n",
    "print(f\"Loaded {len(ltv_features_df)} customer feature sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling: One-hot encode categorical features\n",
    "df_model_data = pd.get_dummies(ltv_features_df, columns=['PREFERRED_CHANNEL'], drop_first=True)\n",
    "df_model_data.set_index('CUSTOMER_ID', inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_model_data.drop('LTV_TARGET', axis=1)\n",
    "y = df_model_data['LTV_TARGET']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data prepared for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosting Regressor model\n",
    "print(\"Training LTV prediction model...\")\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = gbr.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(f\"Model evaluation complete. Test RMSE: ${rmse:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}